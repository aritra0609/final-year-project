ğŸ§  Generative AI Chatbot with RAG (Retrieval-Augmented Generation)
ğŸ” Real-time, intelligent, and ultra-fast chatbot using Groq, OpenAI/Mistral, LangChain, Tavily, and FastAPI.

ğŸ“Œ Overview
This project builds a Generative AI Chatbot capable of producing context-aware, real-time answers using RAG (Retrieval-Augmented Generation). It blends language models with live web search, offering ultra-fast responses thanks to Groq inference and structured agent reasoning via LangGraph.

âœ… Built using Python, integrates LLMs, tools, web APIs, and a full-stack deployment pipeline.

ğŸ’¬ Sample Queries:
â€¢ What is the latest update on OpenAI's GPT models?
â€¢ Who won the last Champions League match?
â€¢ Write SQL query to get top 3 employees by salary.

ğŸ§© Tech Stack
Layer	Technology
Frontend	Streamlit â€“ Chat UI with checkbox for web search
Backend	FastAPI â€“ High-performance API server
LLMs	OpenAI GPT, Mistral â€“ Language generation
Inference	Groq â€“ Accelerated inference engine
Tooling	Tavily API â€“ Real-time web search
Memory + Tools	LangChain + LangGraph ReactAgent
Server	Uvicorn â€“ ASGI server
Validation	Pydantic â€“ Input schema validation
Environment	pipenv â€“ Dependency and virtualenv management

ğŸ”§ Features
âœ… Retrieval-Augmented Generation (RAG) pipeline
ğŸŒ Real-time web search with optional toggle (via Tavily API)
âš¡ Ultra-fast inference using Groqâ€™s hardware-accelerated backend
ğŸ§  Step-by-step reasoning via LangGraph ReactAgent
ğŸ›¡ï¸ Pydantic validation for safe and reliable API usage
ğŸ–¥ï¸ Streamlit UI for interactive chatting
ğŸ” Full frontend-backend integration

ğŸ› ï¸ Project Structure

â”œâ”€â”€ ai_agent.py          # Core logic: model, tools, LangGraph agent
â”œâ”€â”€ backend.py           # FastAPI backend, input validation, routing
â”œâ”€â”€ frontend.py          # Streamlit UI, handles user chat and checkboxes
â”œâ”€â”€ Pipfile              # Dependencies and virtualenv definition
â”œâ”€â”€ README.md            # Youâ€™re here!
ğŸ“ˆ How It Works

User inputs a query in Streamlit, and optionally enables â€œWeb Searchâ€.
FastAPI receives the query and passes it to the AI agent.
LangGraph ReactAgent decides how to respond:
Uses Tavily tool if needed (based on checkbox)
Uses Groq-accelerated LLM (OpenAI/Mistral)
Or combines both (RAG)
Final response is returned to the frontend UI.

ğŸ“Œ Key Components
ğŸ”¹ Groq
Hardware-accelerated inference engine for ultra-low latency LLM output.

ğŸ”¹ OpenAI / Mistral
Supports both proprietary (GPT) and open-source (Mistral) models.

ğŸ”¹ Tavily API
Enables real-time search; useful for time-sensitive or web-based queries.

ğŸ”¹ LangChain + LangGraph
Memory, reasoning steps, and tool orchestration via ReactAgent flow.

ğŸ”¹ FastAPI + Uvicorn
Backend to serve AI responses with speed, stability, and validation.

ğŸ§  Learnings
Implemented a full-stack AI app with custom tool orchestration.
Understood LangChain + LangGraph agent framework deeply.
Integrated real-time search, validation, and modular API backend.
Used Groq for optimizing latency-sensitive inference.

ğŸ¯ Future Enhancements
ğŸ™ï¸ Add voice input/output
ğŸŒ Enable multilingual support
ğŸ–¼ï¸ Add image input (multimodal)
ğŸ‘¤ Add user profile personalization

ğŸ§ª API Endpoints

POST /chat
{
  "query": "Who is the president of India?",
  "allow_search": true
}
Response:
{
  "response": "As of June 2025, the President of India is..."
}
